{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/aurora-roundhacks/new_train.csv')\ndf1=pd.read_csv('../input/aurora-roundhacks/new_test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['date'].dtypes\ndf['date'] = pd.to_datetime(df['date'])\n\ndf['Year'] = df['date'].dt.year\ndf['Month'] = df['date'].dt.month\ndf['Day'] = df['date'].dt.day\n\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['date'], axis=1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(['temp'], axis=1)\n\ny = df['temp']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['date'] = pd.to_datetime(df1['date'])\n\ndf1['Year'] = df1['date'].dt.year\ndf1['Month'] = df1['date'].dt.month\ndf1['Day'] = df1['date'].dt.day\n\ndf1.drop(['date'], axis=1, inplace = True)\n\ndf1.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install catboost\n!pip install optuna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgbm\n\nimport optuna\nfrom optuna import Trial, visualization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# making 5 splits\nkf = KFold(n_splits=5, shuffle=True, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGB_params={'colsample_bytree': 1.0,\n 'eta': 0.033634380372389666,\n 'max_depth': 11,\n 'min_child_weight': 6,\n 'n_estimators': 500,\n 'subsample': 0.8}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost_params={'learning_rate': 0.34572681009606276,\n 'max_depth': 13,\n 'n_estimators': 150,\n 'subsample': 0.9}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rat_params={'max_depth': 15, 'max_features': 3, 'max_samples': 0.8, 'n_estimators': 1500}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbreg = lgbm.LGBMRegressor(device_type='gpu')\n\n# taking positive because cross val score returns -ve values\nscores = np.abs(cross_val_score(lgbreg, X, y, cv=kf, scoring='neg_root_mean_squared_error'))\n\n# printing the output\nprint(f\"Scores - {scores}\\nMean - {sum(scores)/len(scores)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Objective(trial):\n    params = {\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 12.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 12.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    model = lgbm.LGBMRegressor(**params, device='GPU')\n    \n    \n    # taking positive because cross val score returns -ve values\n    scores = np.abs(cross_val_score(model, X, y, cv=kf, scoring='neg_root_mean_squared_error'))\n    \n    return sum(scores)/len(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"minimize\", study_name='LGBM optimization')\nstudy.optimize(Objective, n_trials=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = study.best_params\nstudy.best_params\nlgb_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\n\nfinal_estimator = GradientBoostingRegressor(n_estimators=350, \n                                            random_state=42)\n\nestimators = [('xgb', XGBRegressor(tree_method='gpu_hist', **XGB_params)),\n              ('lgb', lgbm.LGBMRegressor(device_type='gpu',**lgb_params)), \n               ('rat',RandomForestRegressor(**rat_params)),\n              ('cat', CatBoostRegressor(verbose=0, task_type='GPU', **catboost_params))] \n\nreg = StackingRegressor(\n        estimators=estimators,\n         final_estimator=final_estimator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taking positive because cross val score returns -ve values\nscores = np.abs(cross_val_score(reg, X, y.values.ravel(), cv=kf, scoring='neg_root_mean_squared_error'))\n\n# printing the output\nprint(f\"Scores - {scores}\\nMean - {sum(scores)/len(scores)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg.fit(X, y.values.ravel())\ny_pred = reg.predict(df1)\ny_pred=pd.DataFrame(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.to_csv('Sub12.csv',header=['prediction'],index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb=lgbm.LGBMRegressor(device_type='gpu',lambda_l1= 0.003005391918043113,lambda_l2=8.006108609364085e-07,\nnum_leaves= 166,feature_fraction= 0.8470174798951228,bagging_fraction= 0.9267780669292209,bagging_freq= 7,min_child_samples= 17)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\naccuracy=[]\nprediction=0\n\nskf=KFold(n_splits=10, random_state=None)\n\nfor train_index, test_index in skf.split(X,y):\n       X1_train,X1_test=X.iloc[train_index], X.iloc[test_index]\n       Y1_train,Y1_test=y.iloc[train_index], y.iloc[test_index]\n\n       lgb.fit(X1_train,Y1_train)\n       prediction+=lgb.predict(df1)/10\n\nprediction=pd.DataFrame(prediction)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction.to_csv('Sub13.csv',header=['prediction'],index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrag = RandomForestRegressor()\n\n# taking positive because cross val score returns -ve values\nscores = np.abs(cross_val_score(rag, X, y.values.ravel(), cv=kf, scoring='neg_root_mean_squared_error'))\n\n# printing the output\nprint(f\"Scores - {scores}\\nMean - {sum(scores)/len(scores)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [50,100,125,250,500,750,1000,1250,1500,2000]),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\",[5,7,9,11,13,15,17]),\n        \"max_samples\": trial.suggest_discrete_uniform(\"max_samples\", 0.1,0.8,0.1),\n        \"max_features\": trial.suggest_categorical(\"max_features\",[1,2,3]),\n        \"criterion\":'mse',\n        #\"colsample_bylevel\": trial.suggest_categorial(\"colsample_bylevel\", [0.3,0.4,0.5,0.6,0.7,0.8])\n        # the colsample_bylevel also improves the score a lot, but its commented out because its not supported on GPU currently.\n        # If you are interested, do fork the kernel, and try it out. I reached a score of 0.7004 while tuning with it.\n        \"random_state\": 2021\n    }\n    \n    \n    model = RandomForestRegressor(**params)\n    \n    \n    # taking positive because cross val score returns -ve values\n    scores = np.abs(cross_val_score(model,X , y.values.ravel(), cv=kf, scoring='neg_root_mean_squared_error'))\n\n    return sum(scores)/len(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"minimize\", study_name='RandomForest optimization')\nstudy.optimize(Objective, n_trials=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rat1=study.best_params\nstudy.best_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rat_params1={'n_estimators': 1000, 'max_depth': 17, 'max_samples': 0.8, 'max_features': 3}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rag = RandomForestRegressor(n_estimators= 1000, max_depth= 17, max_samples= 0.8, max_features=3)\nrag.fit(X,y)\ny_pred=rag.predict(df1)\ny_pred=pd.DataFrame(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.to_csv('Sub14.csv',header=['prediction'],index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\naccuracy=[]\nprediction1=0\n\nskf=KFold(n_splits=5, random_state=None)\n\nfor train_index, test_index in skf.split(X,y):\n       X1_train,X1_test=X.iloc[train_index], X.iloc[test_index]\n       Y1_train,Y1_test=y.iloc[train_index], y.iloc[test_index]\n\n       rag.fit(X1_train,Y1_train)\n       prediction1+=rag.predict(df1)/5\n\nprediction1=pd.DataFrame(prediction1)  \nprediction1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction1.to_csv('Sub15.csv',header=['prediction'],index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb1={'lambda_l1': 0.003005391918043113,\n 'lambda_l2': 8.006108609364085e-07,\n 'num_leaves': 166,\n 'feature_fraction': 0.8470174798951228,\n 'bagging_fraction': 0.9267780669292209,\n 'bagging_freq': 7,\n 'min_child_samples': 17}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\n\nfinal_estimator = GradientBoostingRegressor(n_estimators=150, \n                                            random_state=42)\n\nestimators = [('xgb', XGBRegressor(tree_method='gpu_hist', **XGB_params)),\n              ('lgb', lgbm.LGBMRegressor(device_type='gpu',**lgb1)), \n               ('rat',RandomForestRegressor(**rat1))] \n\nreg2 = StackingRegressor(\n        estimators=estimators,\n         final_estimator=final_estimator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\naccuracy=[]\nprediction2=0\n\nskf=KFold(n_splits=5, random_state=None)\n\nfor train_index, test_index in skf.split(X,y):\n       X1_train,X1_test=X.iloc[train_index], X.iloc[test_index]\n       Y1_train,Y1_test=y.iloc[train_index], y.iloc[test_index]\n\n       reg2.fit(X1_train,Y1_train)\n       prediction2+=reg2.predict(df1)/5\n\nprediction2=pd.DataFrame(prediction2)  \nprediction2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction2.to_csv('Sub17.csv',header=['prediction'],index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb1={'lambda_l1': 0.003005391918043113,\n 'lambda_l2': 8.006108609364085e-07,\n 'num_leaves': 166,\n 'feature_fraction': 0.8470174798951228,\n 'bagging_fraction': 0.9267780669292209,\n 'bagging_freq': 7,\n 'min_child_samples': 17}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rat_params1={'n_estimators': 1000, 'max_depth': 17, 'max_samples': 0.8, 'max_features': 3}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrag = RandomForestRegressor(n_estimators= 1000, max_depth= 17, max_samples= 0.8, max_features=3)\n\nfrom sklearn.model_selection import KFold\n\naccuracy=[]\nprediction_rat=0\n\nskf=KFold(n_splits=5, random_state=None)\n\nfor train_index, test_index in skf.split(X,y):\n       X1_train,X1_test=X.iloc[train_index], X.iloc[test_index]\n       Y1_train,Y1_test=y.iloc[train_index], y.iloc[test_index]\n\n       rag.fit(X1_train,Y1_train)\n       prediction_rat+=rag.predict(df1)/5\n\n#prediction_rat=pd.DataFrame(prediction1)  \n#prediction1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lag=lgbm.LGBMRegressor(device_type='gpu',**lgb1)\n\nfrom sklearn.model_selection import KFold\n\naccuracy=[]\nprediction_lgb=0\n\nskf=KFold(n_splits=10, random_state=None)\n\nfor train_index, test_index in skf.split(X,y):\n       X1_train,X1_test=X.iloc[train_index], X.iloc[test_index]\n       Y1_train,Y1_test=y.iloc[train_index], y.iloc[test_index]\n\n       lag.fit(X1_train,Y1_train)\n       prediction_lgb+=lag.predict(df1)/10\n\n#prediction=pd.DataFrame(prediction1)  \n#prediction1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb1=XGBRegressor(tree_method='gpu_hist', **XGB_params)\n\nfrom sklearn.model_selection import KFold\n\naccuracy=[]\nprediction_xgb=0\n\nskf=KFold(n_splits=10, random_state=None)\n\nfor train_index, test_index in skf.split(X,y):\n       X1_train,X1_test=X.iloc[train_index], X.iloc[test_index]\n       Y1_train,Y1_test=y.iloc[train_index], y.iloc[test_index]\n\n       xgb1.fit(X1_train,Y1_train)\n       prediction_xgb+=xgb1.predict(df1)/10\n\n#prediction1=pd.DataFrame(prediction1)  \n#prediction1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat=CatBoostRegressor(verbose=0, task_type='GPU', **catboost_params)\n\nfrom sklearn.model_selection import KFold\n\naccuracy=[]\nprediction_cat=0\n\nskf=KFold(n_splits=10, random_state=None)\n\nfor train_index, test_index in skf.split(X,y):\n       X1_train,X1_test=X.iloc[train_index], X.iloc[test_index]\n       Y1_train,Y1_test=y.iloc[train_index], y.iloc[test_index]\n\n       cat.fit(X1_train,Y1_train)\n       prediction_cat+=cat.predict(df1)/10\n\n#prediction_cat=pd.DataFrame(prediction1)  \n#prediction_cat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_avg=(0.2*prediction_rat+0.6*prediction_lgb+0.1*prediction_xgb+0.1*prediction_cat)\nfinal_avg=pd.DataFrame(final_avg)\nfinal_avg\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_avg.to_csv('Sub54.csv',header=['prediction'],index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}